{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I27mHZWkyYH5",
        "outputId": "1d84881b-bfa9-4bcb-b0ae-5e0f1f6f193e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSuccessfully imported libraries. Using MedMNIST version: 3.0.2\n",
            "INFO: The data_flag is set to: 'breastmnist'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 560k/560k [00:00<00:00, 658kB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Datasets and Dataloaders are ready.\n",
            "INFO: ResNet-18 model is defined and moved to device: cuda:0\n",
            "INFO: Starting training for 3 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 [Train]: 100%|██████████| 5/5 [00:01<00:00,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Validation AUC: 0.662, Accuracy: 0.731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 [Train]: 100%|██████████| 5/5 [00:00<00:00, 26.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 - Validation AUC: 0.642, Accuracy: 0.731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 [Train]: 100%|██████████| 5/5 [00:00<00:00, 26.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 - Validation AUC: 0.679, Accuracy: 0.744\n",
            "\n",
            "INFO: Training complete!\n",
            "INFO: Evaluating model on the final test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Test]: 100%|██████████| 2/2 [00:00<00:00, 35.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Final Test Results for ResNet-18:\n",
            "Test AUC: 0.664\n",
            "Test Accuracy: 0.724\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Install and Import ---\n",
        "# We run the installation quietly with `-q`\n",
        "!pip install -q medmnist\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"Successfully imported libraries. Using MedMNIST version: {medmnist.__version__}\")\n",
        "\n",
        "# in the old style, I would use \"print(\"Successfully imported libraries. Using MedMNIST version: %s\" % (medmnist.__version__))\"\n",
        "\n",
        "# --- 2. Setup Dataset and Dataloaders ---\n",
        "\n",
        "data_flag = 'breastmnist'\n",
        "print(f\"INFO: The data_flag is set to: '{data_flag}'\") # Debug print\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "download = True\n",
        "\n",
        "# Get dataset information from the medmnist library\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "# Define transformations to apply to the images\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "# Load the datasets using the predefined splits and apply the transformations\n",
        "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
        "val_dataset = DataClass(split='val', transform=data_transform, download=download)\n",
        "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
        "\n",
        "# Create DataLoaders to handle batching and shuffling\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"INFO: Datasets and Dataloaders are ready.\")\n",
        "\n",
        "# --- 3. Define and Prepare the Model ---\n",
        "# Use ResNet-18 and train from scratch (pretrained=False)\n",
        "model = resnet18(pretrained=False)\n",
        "\n",
        "# Adjust the model for our specific dataset (1-channel grayscale images)\n",
        "model.conv1 = nn.Conv2d(n_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "# Adjust the final layer for our number of classes (2 for BreastMNIST)\n",
        "model.fc = nn.Linear(model.fc.in_features, n_classes)\n",
        "\n",
        "# Move the model to the GPU if one is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"INFO: ResNet-18 model is defined and moved to device: {device}\")\n",
        "\n",
        "# --- 4. Define Loss, Optimizer, and Training Parameters ---\n",
        "NUM_EPOCHS = 3 # Set to a low number for a quick test. Increase to 100 for full replication.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "print(f\"INFO: Starting training for {NUM_EPOCHS} epochs...\")\n",
        "\n",
        "# --- 5. Training and Validation Loop ---\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Train]\"):\n",
        "        inputs, targets = inputs.to(device), targets.to(device).squeeze().long()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    y_true_val, y_score_val = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            y_true_val.extend(targets.squeeze().tolist())\n",
        "            y_score_val.extend(outputs.softmax(dim=-1).cpu().numpy())\n",
        "\n",
        "    # convert the list of arrays into a single NumPy array\n",
        "    y_score_val = np.vstack(y_score_val)\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_evaluator = Evaluator(data_flag, 'val')\n",
        "    metrics = val_evaluator.evaluate(y_score_val)\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Validation AUC: {metrics[0]:.3f}, Accuracy: {metrics[1]:.3f}\")\n",
        "\n",
        "print(\"\\nINFO: Training complete!\")\n",
        "\n",
        "# --- 6. Final Evaluation on Test Set ---\n",
        "print(\"INFO: Evaluating model on the final test set...\")\n",
        "model.eval()\n",
        "y_true_test, y_score_test = [], []\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in tqdm(test_loader, desc=\"[Test]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        y_true_test.extend(targets.squeeze().tolist())\n",
        "        y_score_test.extend(outputs.softmax(dim=-1).cpu().numpy())\n",
        "\n",
        "y_score_test = np.vstack(y_score_test)\n",
        "\n",
        "# Calculate final test metrics\n",
        "test_evaluator = Evaluator(data_flag, 'test')\n",
        "test_auc, test_acc = test_evaluator.evaluate(y_score_test)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Final Test Results for ResNet-18:\")\n",
        "print(f\"Test AUC: {test_auc:.3f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "R1Z05KCdxrBq",
        "outputId": "afd0eb25-61d1-44ef-85b3-54cb6ac2da81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: {'pathmnist': {'python_class': 'PathMNIST', 'description': 'The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.', 'url': 'https://zenodo.org/records/10519652/files/pathmnist.npz?download=1', 'MD5': 'a8b06965200029087d5bd730944a56c1', 'url_64': 'https://zenodo.org/records/10519652/files/pathmnist_64.npz?download=1', 'MD5_64': '55aa9c1e0525abe5a6b9d8343a507616', 'url_128': 'https://zenodo.org/records/10519652/files/pathmnist_128.npz?download=1', 'MD5_128': 'ac42d08fb904d92c244187169d1fd1d9', 'url_224': 'https://zenodo.org/records/10519652/files/pathmnist_224.npz?download=1', 'MD5_224': '2c51a510bcdc9cf8ddb2af93af1eadec', 'task': 'multi-class', 'label': {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}, 'n_channels': 3, 'n_samples': {'train': 89996, 'val': 10004, 'test': 7180}, 'license': 'CC BY 4.0'}, 'chestmnist': {'python_class': 'ChestMNIST', 'description': 'The ChestMNIST is based on the NIH-ChestXray14 dataset, a dataset comprising 112,120 frontal-view X-Ray images of 30,805 unique patients with the text-mined 14 disease labels, which could be formulized as a multi-label binary-class classification task. We use the official data split, and resize the source images of 1×1024×1024 into 1×28×28.', 'url': 'https://zenodo.org/records/10519652/files/chestmnist.npz?download=1', 'MD5': '02c8a6516a18b556561a56cbdd36c4a8', 'url_64': 'https://zenodo.org/records/10519652/files/chestmnist_64.npz?download=1', 'MD5_64': '9de6cd0b934ebb5b7426cfba5efbae16', 'url_128': 'https://zenodo.org/records/10519652/files/chestmnist_128.npz?download=1', 'MD5_128': 'db107e5590b27930b62dbaf558aebee3', 'url_224': 'https://zenodo.org/records/10519652/files/chestmnist_224.npz?download=1', 'MD5_224': '45bd33e6f06c3e8cdb481c74a89152aa', 'task': 'multi-label, binary-class', 'label': {'0': 'atelectasis', '1': 'cardiomegaly', '2': 'effusion', '3': 'infiltration', '4': 'mass', '5': 'nodule', '6': 'pneumonia', '7': 'pneumothorax', '8': 'consolidation', '9': 'edema', '10': 'emphysema', '11': 'fibrosis', '12': 'pleural', '13': 'hernia'}, 'n_channels': 1, 'n_samples': {'train': 78468, 'val': 11219, 'test': 22433}, 'license': 'CC BY 4.0'}, 'dermamnist': {'python_class': 'DermaMNIST', 'description': 'The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.', 'url': 'https://zenodo.org/records/10519652/files/dermamnist.npz?download=1', 'MD5': '0744692d530f8e62ec473284d019b0c7', 'url_64': 'https://zenodo.org/records/10519652/files/dermamnist_64.npz?download=1', 'MD5_64': 'b70a2f5635c6199aeaa28c31d7202e1f', 'url_128': 'https://zenodo.org/records/10519652/files/dermamnist_128.npz?download=1', 'MD5_128': '2defd784463fa5243564e855ed717de1', 'url_224': 'https://zenodo.org/records/10519652/files/dermamnist_224.npz?download=1', 'MD5_224': '8974907d8e169bef5f5b96bc506ae45d', 'task': 'multi-class', 'label': {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}, 'n_channels': 3, 'n_samples': {'train': 7007, 'val': 1003, 'test': 2005}, 'license': 'CC BY-NC 4.0'}, 'octmnist': {'python_class': 'OCTMNIST', 'description': 'The OCTMNIST is based on a prior dataset of 109,309 valid optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the source training set with a ratio of 9:1 into training and validation set, and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−1,536)×(277−512). We center-crop the images and resize them into 1×28×28.', 'url': 'https://zenodo.org/records/10519652/files/octmnist.npz?download=1', 'MD5': 'c68d92d5b585d8d81f7112f81e2d0842', 'url_64': 'https://zenodo.org/records/10519652/files/octmnist_64.npz?download=1', 'MD5_64': 'e229e9440236b774d9f0dfef9d07bdaf', 'url_128': 'https://zenodo.org/records/10519652/files/octmnist_128.npz?download=1', 'MD5_128': '0a97e76651ace45c5d943ee3f65b63ae', 'url_224': 'https://zenodo.org/records/10519652/files/octmnist_224.npz?download=1', 'MD5_224': 'abc493b6d529d5de7569faaef2773ba3', 'task': 'multi-class', 'label': {'0': 'choroidal neovascularization', '1': 'diabetic macular edema', '2': 'drusen', '3': 'normal'}, 'n_channels': 1, 'n_samples': {'train': 97477, 'val': 10832, 'test': 1000}, 'license': 'CC BY 4.0'}, 'pneumoniamnist': {'python_class': 'PneumoniaMNIST', 'description': 'The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.', 'url': 'https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1', 'MD5': '28209eda62fecd6e6a2d98b1501bb15f', 'url_64': 'https://zenodo.org/records/10519652/files/pneumoniamnist_64.npz?download=1', 'MD5_64': '8f4eceb4ccffa70c672198ea285246c6', 'url_128': 'https://zenodo.org/records/10519652/files/pneumoniamnist_128.npz?download=1', 'MD5_128': '05b46931834c231683c68f40c47b2971', 'url_224': 'https://zenodo.org/records/10519652/files/pneumoniamnist_224.npz?download=1', 'MD5_224': 'd6a3c71de1b945ea11211b03746c1fe1', 'task': 'binary-class', 'label': {'0': 'normal', '1': 'pneumonia'}, 'n_channels': 1, 'n_samples': {'train': 4708, 'val': 524, 'test': 624}, 'license': 'CC BY 4.0'}, 'retinamnist': {'python_class': 'RetinaMNIST', 'description': 'The RetinaMNIST is based on the DeepDRiD challenge, which provides a dataset of 1,600 retina fundus images. The task is ordinal regression for 5-level grading of diabetic retinopathy severity. We split the source training set with a ratio of 9:1 into training and validation set, and use the source validation set as the test set. The source images of 3×1,736×1,824 are center-cropped and resized into 3×28×28.', 'url': 'https://zenodo.org/records/10519652/files/retinamnist.npz?download=1', 'MD5': 'bd4c0672f1bba3e3a89f0e4e876791e4', 'url_64': 'https://zenodo.org/records/10519652/files/retinamnist_64.npz?download=1', 'MD5_64': 'afda852cc34dcda56f86ad2b2457dbcc', 'url_128': 'https://zenodo.org/records/10519652/files/retinamnist_128.npz?download=1', 'MD5_128': 'e48e916a24454daf90583d4e6efb1a18', 'url_224': 'https://zenodo.org/records/10519652/files/retinamnist_224.npz?download=1', 'MD5_224': 'eae7e3b6f3fcbda4ae613ebdcbe35348', 'task': 'ordinal-regression', 'label': {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4'}, 'n_channels': 3, 'n_samples': {'train': 1080, 'val': 120, 'test': 400}, 'license': 'CC BY 4.0'}, 'breastmnist': {'python_class': 'BreastMNIST', 'description': 'The BreastMNIST is based on a dataset of 780 breast ultrasound images. It is categorized into 3 classes: normal, benign, and malignant. As we use low-resolution images, we simplify the task into binary classification by combining normal and benign as positive and classifying them against malignant as negative. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images of 1×500×500 are resized into 1×28×28.', 'url': 'https://zenodo.org/records/10519652/files/breastmnist.npz?download=1', 'MD5': '750601b1f35ba3300ea97c75c52ff8f6', 'url_64': 'https://zenodo.org/records/10519652/files/breastmnist_64.npz?download=1', 'MD5_64': '742edef2a1fd1524b2efff4bd7ba9364', 'url_128': 'https://zenodo.org/records/10519652/files/breastmnist_128.npz?download=1', 'MD5_128': '363e4b3f8d712e9b5de15470a2aaadf1', 'url_224': 'https://zenodo.org/records/10519652/files/breastmnist_224.npz?download=1', 'MD5_224': 'b56378a6eefa9fed602bb16d192d4c8b', 'task': 'binary-class', 'label': {'0': 'malignant', '1': 'normal, benign'}, 'n_channels': 1, 'n_samples': {'train': 546, 'val': 78, 'test': 156}, 'license': 'CC BY 4.0'}, 'bloodmnist': {'python_class': 'BloodMNIST', 'description': 'The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.', 'url': 'https://zenodo.org/records/10519652/files/bloodmnist.npz?download=1', 'MD5': '7053d0359d879ad8a5505303e11de1dc', 'url_64': 'https://zenodo.org/records/10519652/files/bloodmnist_64.npz?download=1', 'MD5_64': '2b94928a2ae4916078ca51e05b6b800b', 'url_128': 'https://zenodo.org/records/10519652/files/bloodmnist_128.npz?download=1', 'MD5_128': 'adace1e0ed228fccda1f39692059dd4c', 'url_224': 'https://zenodo.org/records/10519652/files/bloodmnist_224.npz?download=1', 'MD5_224': 'b718ff6835fcbdb22ba9eacccd7b2601', 'task': 'multi-class', 'label': {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}, 'n_channels': 3, 'n_samples': {'train': 11959, 'val': 1712, 'test': 3421}, 'license': 'CC BY 4.0'}, 'tissuemnist': {'python_class': 'TissueMNIST', 'description': 'We use the BBBC051, available from the Broad Bioimage Benchmark Collection. The dataset contains 236,386 human kidney cortex cells, segmented from 3 reference tissue specimens and organized into 8 categories. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. Each gray-scale image is 32×32×7 pixels, where 7 denotes 7 slices. We take maximum values across the slices and resize them into 28×28 gray-scale images.', 'url': 'https://zenodo.org/records/10519652/files/tissuemnist.npz?download=1', 'MD5': 'ebe78ee8b05294063de985d821c1c34b', 'url_64': 'https://zenodo.org/records/10519652/files/tissuemnist_64.npz?download=1', 'MD5_64': '123ece2eba09d0aa5d698fda57103344', 'url_128': 'https://zenodo.org/records/10519652/files/tissuemnist_128.npz?download=1', 'MD5_128': '61b955355d7425a89687b06cca3ce0c2', 'url_224': 'https://zenodo.org/records/10519652/files/tissuemnist_224.npz?download=1', 'MD5_224': 'b077128c4a949f0a4eb01517f9037b9c', 'task': 'multi-class', 'label': {'0': 'Collecting Duct, Connecting Tubule', '1': 'Distal Convoluted Tubule', '2': 'Glomerular endothelial cells', '3': 'Interstitial endothelial cells', '4': 'Leukocytes', '5': 'Podocytes', '6': 'Proximal Tubule Segments', '7': 'Thick Ascending Limb'}, 'n_channels': 1, 'n_samples': {'train': 165466, 'val': 23640, 'test': 47280}, 'license': 'CC BY 4.0'}, 'organamnist': {'python_class': 'OrganAMNIST', 'description': 'The OrganAMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Axial (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in axial views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.', 'url': 'https://zenodo.org/records/10519652/files/organamnist.npz?download=1', 'MD5': '68e3f8846a6bd62f0c9bf841c0d9eacc', 'url_64': 'https://zenodo.org/records/10519652/files/organamnist_64.npz?download=1', 'MD5_64': '2dcccc29b88e6da5a01161ef20cda288', 'url_128': 'https://zenodo.org/records/10519652/files/organamnist_128.npz?download=1', 'MD5_128': 'eeae80d0a227a8d099027e1b3cfd3b60', 'url_224': 'https://zenodo.org/records/10519652/files/organamnist_224.npz?download=1', 'MD5_224': '50747347e05c87dd3aaf92c49f9f3170', 'task': 'multi-class', 'label': {'0': 'bladder', '1': 'femur-left', '2': 'femur-right', '3': 'heart', '4': 'kidney-left', '5': 'kidney-right', '6': 'liver', '7': 'lung-left', '8': 'lung-right', '9': 'pancreas', '10': 'spleen'}, 'n_channels': 1, 'n_samples': {'train': 34561, 'val': 6491, 'test': 17778}, 'license': 'CC BY 4.0'}, 'organcmnist': {'python_class': 'OrganCMNIST', 'description': 'The OrganCMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Coronal (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in coronal views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.', 'url': 'https://zenodo.org/records/10519652/files/organcmnist.npz?download=1', 'MD5': 'b9ceb9546e10131b32923c5bbeaea2b1', 'url_64': 'https://zenodo.org/records/10519652/files/organcmnist_64.npz?download=1', 'MD5_64': '3ce34a8724ea6f548e6db4744d03b6a9', 'url_128': 'https://zenodo.org/records/10519652/files/organcmnist_128.npz?download=1', 'MD5_128': '773c1f009daa3fe5d9a2a201b2a7ed94', 'url_224': 'https://zenodo.org/records/10519652/files/organcmnist_224.npz?download=1', 'MD5_224': '050f5e875dc056f6768abf94ec9995d1', 'task': 'multi-class', 'label': {'0': 'bladder', '1': 'femur-left', '2': 'femur-right', '3': 'heart', '4': 'kidney-left', '5': 'kidney-right', '6': 'liver', '7': 'lung-left', '8': 'lung-right', '9': 'pancreas', '10': 'spleen'}, 'n_channels': 1, 'n_samples': {'train': 12975, 'val': 2392, 'test': 8216}, 'license': 'CC BY 4.0'}, 'organsmnist': {'python_class': 'OrganSMNIST', 'description': 'The OrganSMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Sagittal (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in sagittal views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.', 'url': 'https://zenodo.org/records/10519652/files/organsmnist.npz?download=1', 'MD5': '9ab87b696fb54e2a387ebe992d6ed5f1', 'url_64': 'https://zenodo.org/records/10519652/files/organsmnist_64.npz?download=1', 'MD5_64': '53a6d115339d874c25e309a994ff46d3', 'url_128': 'https://zenodo.org/records/10519652/files/organsmnist_128.npz?download=1', 'MD5_128': 'ded0c5fa01a95dc4978b956f613e9b8e', 'url_224': 'https://zenodo.org/records/10519652/files/organsmnist_224.npz?download=1', 'MD5_224': 'b354719e553fbbb2513d5533f52a4cb1', 'task': 'multi-class', 'label': {'0': 'bladder', '1': 'femur-left', '2': 'femur-right', '3': 'heart', '4': 'kidney-left', '5': 'kidney-right', '6': 'liver', '7': 'lung-left', '8': 'lung-right', '9': 'pancreas', '10': 'spleen'}, 'n_channels': 1, 'n_samples': {'train': 13932, 'val': 2452, 'test': 8827}, 'license': 'CC BY 4.0'}, 'organmnist3d': {'python_class': 'OrganMNIST3D', 'description': 'The source of the OrganMNIST3D is the same as that of the Organ{A,C,S}MNIST. Instead of 2D images, we directly use the 3D bounding boxes and process the images into 28×28×28 to perform multi-class classification of 11 body organs. The same 115 and 16 CT scans as the Organ{A,C,S}MNIST from the source training set are used as training and validation set, respectively, and the same 70 CT scans as the Organ{A,C,S}MNIST from the source test set are treated as the test set.', 'url': 'https://zenodo.org/records/10519652/files/organmnist3d.npz?download=1', 'MD5': 'a0c5a1ff56af4f155c46d46fbb45a2fe', 'url_64': 'https://zenodo.org/records/10519652/files/organmnist3d_64.npz?download=1', 'MD5_64': '58a2205adf14a9d0a189cb06dc78bf10', 'task': 'multi-class', 'label': {'0': 'liver', '1': 'kidney-right', '2': 'kidney-left', '3': 'femur-right', '4': 'femur-left', '5': 'bladder', '6': 'heart', '7': 'lung-right', '8': 'lung-left', '9': 'spleen', '10': 'pancreas'}, 'n_channels': 1, 'n_samples': {'train': 971, 'val': 161, 'test': 610}, 'license': 'CC BY 4.0'}, 'nodulemnist3d': {'python_class': 'NoduleMNIST3D', 'description': 'The NoduleMNIST3D is based on the LIDC-IDRI, a large public lung nodule dataset, containing images from thoracic CT scans. The dataset is designed for both lung nodule segmentation and 5-level malignancy classification task. To perform binary classification, we categorize cases with malignancy level 1/2 into negative class and 4/5 into positive class, ignoring the cases with malignancy level 3. We split the source dataset with a ratio of 7:1:2 into training, validation and test set, and center-crop the spatially normalized images (with a spacing of 1mm×1mm×1mm) into 28×28×28.', 'url': 'https://zenodo.org/records/10519652/files/nodulemnist3d.npz?download=1', 'MD5': '8755a7e9e05a4d9ce80a24c3e7a256f3', 'url_64': 'https://zenodo.org/records/10519652/files/nodulemnist3d_64.npz?download=1', 'MD5_64': 'c47c5b7d457bf6332200d2ea6d64ecd8', 'task': 'binary-class', 'label': {'0': 'benign', '1': 'malignant'}, 'n_channels': 1, 'n_samples': {'train': 1158, 'val': 165, 'test': 310}, 'license': 'CC BY 4.0'}, 'adrenalmnist3d': {'python_class': 'AdrenalMNIST3D', 'description': 'The AdrenalMNIST3D is a new 3D shape classification dataset, consisting of shape masks from 1,584 left and right adrenal glands (i.e., 792 patients). Collected from Zhongshan Hospital Affiliated to Fudan University, each 3D shape of adrenal gland is annotated by an expert endocrinologist using abdominal computed tomography (CT), together with a binary classification label of normal adrenal gland or adrenal mass. Considering patient privacy, we do not provide the source CT scans, but the real 3D shapes of adrenal glands and their classification labels. We calculate the center of adrenal and resize the center-cropped 64mm×64mm×64mm volume into 28×28×28. The dataset is randomly split into training/validation/test set of 1,188/98/298 on a patient level.', 'url': 'https://zenodo.org/records/10519652/files/adrenalmnist3d.npz?download=1', 'MD5': 'bbd3c5a5576322bc4cdfea780653b1ce', 'url_64': 'https://zenodo.org/records/10519652/files/adrenalmnist3d_64.npz?download=1', 'MD5_64': '17721accfe9fb005146a47d33bc54b2f', 'task': 'binary-class', 'label': {'0': 'normal', '1': 'hyperplasia'}, 'n_channels': 1, 'n_samples': {'train': 1188, 'val': 98, 'test': 298}, 'license': 'CC BY 4.0'}, 'fracturemnist3d': {'python_class': 'FractureMNIST3D', 'description': 'The FractureMNIST3D is based on the RibFrac Dataset, containing around 5,000 rib fractures from 660 computed tomography 153 (CT) scans. The dataset organizes detected rib fractures into 4 clinical categories (i.e., buckle, nondisplaced, displaced, and segmental rib fractures). As we use low-resolution images, we disregard segmental rib fractures and classify 3 types of rib fractures (i.e., buckle, nondisplaced, and displaced). For each annotated fracture area, we calculate its center and resize the center-cropped 64mm×64mm×64mm image into 28×28×28. The official split of training, validation and test set is used.', 'url': 'https://zenodo.org/records/10519652/files/fracturemnist3d.npz?download=1', 'MD5': '6aa7b0143a6b42da40027a9dda61302f', 'url_64': 'https://zenodo.org/records/10519652/files/fracturemnist3d_64.npz?download=1', 'MD5_64': 'f01d7e6316aedf4210da0da5b7437b42', 'task': 'multi-class', 'label': {'0': 'buckle rib fracture', '1': 'nondisplaced rib fracture', '2': 'displaced rib fracture'}, 'n_channels': 1, 'n_samples': {'train': 1027, 'val': 103, 'test': 240}, 'license': 'CC BY 4.0'}, 'vesselmnist3d': {'python_class': 'VesselMNIST3D', 'description': 'The VesselMNIST3D is based on an open-access 3D intracranial aneurysm dataset, IntrA, containing 103 3D models (meshes) of entire brain vessels collected by reconstructing MRA images. 1,694 healthy vessel segments and 215 aneurysm segments are generated automatically from the complete models. We fix the non-watertight mesh with PyMeshFix and voxelize the watertight mesh with trimesh into 28×28×28 voxels. We split the source dataset with a ratio of 7:1:2 into training, validation and test set.', 'url': 'https://zenodo.org/records/10519652/files/vesselmnist3d.npz?download=1', 'MD5': 'b41fd4f7e7e2feedddb201585ecafa1b', 'url_64': 'https://zenodo.org/records/10519652/files/vesselmnist3d_64.npz?download=1', 'MD5_64': '6bb274a8846e1097066dcd64e2c4520f', 'task': 'binary-class', 'label': {'0': 'vessel', '1': 'aneurysm'}, 'n_channels': 1, 'n_samples': {'train': 1335, 'val': 191, 'test': 382}, 'license': 'CC BY 4.0'}, 'synapsemnist3d': {'python_class': 'SynapseMNIST3D', 'description': 'The SynapseMNIST3D is a new 3D volume dataset to classify whether a synapse is excitatory or inhibitory. It uses a 3D image volume of an adult rat acquired by a multi-beam scanning electron microscope. The original data is of the size 100×100×100um^3 and the resolution 8×8×30nm^3, where a (30um)^3 sub-volume was used in the MitoEM dataset with dense 3D mitochondria instance segmentation labels. Three neuroscience experts segment a pyramidal neuron within the whole volume and proofread all the synapses on this neuron with excitatory/inhibitory labels. For each labeled synaptic location, we crop a 3D volume of 1024×1024×1024nm^3 and resize it into 28×28×28 voxels. Finally, the dataset is randomly split with a ratio of 7:1:2 into training, validation and test set.', 'url': 'https://zenodo.org/records/10519652/files/synapsemnist3d.npz?download=1', 'MD5': '1235b78a3cd6280881dd7850a78eadb6', 'url_64': 'https://zenodo.org/records/10519652/files/synapsemnist3d_64.npz?download=1', 'MD5_64': '43bd14ebf3af9d3dd072446fedc14d5e', 'task': 'binary-class', 'label': {'0': 'inhibitory synapse', '1': 'excitatory synapse'}, 'n_channels': 1, 'n_samples': {'train': 1230, 'val': 177, 'test': 352}, 'license': 'CC BY 4.0'}}\n",
            "Data Class: <class 'medmnist.dataset.BreastMNIST'>\n",
            "Info: {'python_class': 'BreastMNIST', 'description': 'The BreastMNIST is based on a dataset of 780 breast ultrasound images. It is categorized into 3 classes: normal, benign, and malignant. As we use low-resolution images, we simplify the task into binary classification by combining normal and benign as positive and classifying them against malignant as negative. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images of 1×500×500 are resized into 1×28×28.', 'url': 'https://zenodo.org/records/10519652/files/breastmnist.npz?download=1', 'MD5': '750601b1f35ba3300ea97c75c52ff8f6', 'url_64': 'https://zenodo.org/records/10519652/files/breastmnist_64.npz?download=1', 'MD5_64': '742edef2a1fd1524b2efff4bd7ba9364', 'url_128': 'https://zenodo.org/records/10519652/files/breastmnist_128.npz?download=1', 'MD5_128': '363e4b3f8d712e9b5de15470a2aaadf1', 'url_224': 'https://zenodo.org/records/10519652/files/breastmnist_224.npz?download=1', 'MD5_224': 'b56378a6eefa9fed602bb16d192d4c8b', 'task': 'binary-class', 'label': {'0': 'malignant', '1': 'normal, benign'}, 'n_channels': 1, 'n_samples': {'train': 546, 'val': 78, 'test': 156}, 'license': 'CC BY 4.0'}\n",
            "Image Data Type: <class 'numpy.ndarray'>\n",
            "Image Shape: (28, 28)\n",
            "Label Data Type: <class 'numpy.ndarray'>\n",
            "Label Value: [1]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# !pip install -q medmnist\n",
        "# Import necessary libraries\n",
        "# import numpy as np\n",
        "\n",
        "# import medmnist\n",
        "# from medmnist import INFO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download and load the dataset\n",
        "data_flag = 'breastmnist'\n",
        "info = INFO[data_flag]\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "print(f\"INFO: {INFO}\")\n",
        "\n",
        "print(f\"Data Class: {DataClass}\")\n",
        "print(f\"Info: {info}\")\n",
        "\n",
        "train_dataset = DataClass(split='train', download=True)\n",
        "\n",
        "# Get the first image (as a PIL Image) and its label\n",
        "pil_image, label = train_dataset[1]\n",
        "\n",
        "# --- THE FIX ---\n",
        "# Convert the PIL Image to a NumPy array\n",
        "image = np.array(pil_image)\n",
        "\n",
        "# --- Inspect the data (This section will now work correctly) ---\n",
        "print(f\"Image Data Type: {type(image)}\")\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "print(f\"Label Data Type: {type(label)}\")\n",
        "print(f\"Label Value: {label}\")\n",
        "\n",
        "# --- Visualize the data ---\n",
        "# Get the metadata to translate the label\n",
        "label_name = info['label'][str(label[0])]\n",
        "\n",
        "# Plot the image using matplotlib\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {label[0]} ({label_name})\")\n",
        "plt.axis('off') # Hide the grid and axes\n",
        "plt.show()"
      ]
    }
  ]
}